# -*- coding: utf-8 -*-
"""PA2_Q2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Sjoc2dAFZ98EfmigHgBUaiIEaIXvQp6D
"""

import pandas as pd
import importlib
from IPython.display import Audio
import torch
import librosa
import librosa.display
import matplotlib.pyplot as plt
import sys
import soundfile as sf

import kagglehub

# Download latest version
path = kagglehub.dataset_download("hbchaitanyabharadwaj/audio-dataset-with-10-indian-languages")

print("Path to dataset files:", path)

import os
fld = "D:/Sem3/Speech/Assignments/Assignment2/Data/Languages/Language Detection Dataset"
langdf = pd.DataFrame(columns=['lang', 'path'])
lst = []
for root, dirs, files in os.walk(fld):
  if os.path.basename(root) == os.path.basename(fld):
    continue
  folderNm = os.path.basename(root)
  print(folderNm)
  lst = lst + [{'lang': folderNm, 'path': os.path.join(fld,folderNm,f)} for f in files]

langdf = pd.DataFrame(lst)

del lst

# import torchaudio
from pydub import AudioSegment
def getMFCC(file_path):
    audio = AudioSegment.from_file(file_path, format="mp3")
    audio.export("converted.wav", format="wav")
    y, sr = librosa.load("converted.wav", sr=None)#torchaudio.load(file_path, format='mp3', backend='ffmpeg')
    s = librosa.feature.melspectrogram(y=y, sr=sr)
    return s

langsampleLst = langdf.groupby("lang").sample(n=1, random_state=1).values.tolist()

fig, axs = plt.subplots(5, 2, sharey=True)
fig.set_size_inches(10,15)
allAx = axs.flat
# baseLen = 100*60
# plt.figure(figsize=(10, 4))
# basePath = "./Q1/"
for ax, f in zip(allAx,langsampleLst):
    lang, file_path = f
    # fzcr, fste, fmfccs, ffrcount = featureExtractor(fnm)
    # librosa.display.specshow(fmfccs, ax=ax)
    audio = AudioSegment.from_file(file_path, format="mp3")
    audio.export("converted.wav", format="wav")
    y, sr = librosa.load("converted.wav", sr=None)#torchaudio.load(file_path, format='mp3', backend='ffmpeg')
    s = librosa.feature.melspectrogram(y=y, sr=sr)
    # D = librosa.amplitude_to_db(librosa.stft(y), ref=np.max)
    # librosa.display.specshow(D, x_axis='time',y_axis="log", ax=ax)
    librosa.display.specshow(s, ax=ax, y_axis='mel', cmap='plasma')
    ax.set_title(lang)
    # ax.set_xlabel('Time')
    # ax.set_ylabel('Frequency')

import numpy as np
sampledf = langdf.groupby("lang").sample(n=1000, random_state=1)
sampledf['mel'] = sampledf.apply(lambda r: getMFCC(r['path']), axis = 1)
sampledf['dimSize'] = sampledf.apply(lambda r: np.array(r['mel']).shape[1], axis = 1)
sampledf = sampledf.loc[sampledf["dimSize"]>=400]
sampledf['newmel'] = sampledf.apply(lambda r: np.mean(np.array(r['mel'])[:,:400], axis=1).T, axis = 1)
sampledf.reset_index(drop=True, inplace=True)
# sampledf['shape'] = sampledf['newmel'].apply(lambda x: x.shape)

tdfMean = sampledf.groupby('lang')['newmel'].agg(np.mean)
tdfVar = sampledf.groupby('lang')['newmel'].apply(lambda x: np.var(np.stack(x.to_numpy()), axis=0))

# tdf.loc['Hindi'].shape
means = tdfMean.to_dict()
vars = tdfVar.to_dict()

# means['Hindi'].shape
for k, m in means.items():
    plt.plot(m, label=k)

plt.title('Mean of 128 MFCC Coef per language')
plt.xlabel('MFCC')
plt.ylabel('Mean')
plt.legend()
plt.show()

for k, v in vars.items():
    plt.plot(v, label=k)

plt.title('Variance of 128 MFCC Coef per language')
plt.xlabel('MFCC')
plt.ylabel('Variance')
plt.legend()
plt.show()
# means

idxLst = list(tdfMean.index)
sampledf['aud_id'] = sampledf['lang'].apply(lambda x: idxLst.index(x))
sampledf['melflat'] = sampledf.apply(lambda r:  np.array(r['mel'])[:,:400].flatten(), axis = 1)

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import classification_report, accuracy_score
from sklearn.metrics import confusion_matrix
import seaborn as sns

fdf = sampledf['melflat'].values.tolist()
X_train, X_test, y_train, y_test = train_test_split(fdf, sampledf['aud_id'], test_size=0.2, random_state=45, stratify=sampledf['aud_id'])

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# classifier = SVC(kernel='rbf', random_state=45, decision_function_shape='ovo')
# classifier.fit(X_train, y_train)
# y_pred = classifier.predict(X_test)

# calc_classification_report = classification_report(y_test, y_pred, target_names=label_encoder.classes_)
# print("Classification Report:\n", calc_classification_report)

# calc_confusion_matrix = confusion_matrix(y_test, y_pred)
# print("Confusion matrix:\n", calc_confusion_matrix)

# plt.figure(figsize=(6, 5))
# sns.heatmap(calc_confusion_matrix, annot=True, cmap="Blues", xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_, fmt='g')
# plt.xlabel("Predicted Label")
# plt.ylabel("True Label")
# plt.title("Confusion Matrix (KNN)")
# plt.show()

from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
print(f'Accuracy: {accuracy * 100:.2f}%')

# Print a detailed classification report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))



# print("\nDataset Sizes:")
# print(f"Total samples: {len(X)}")
# print(f"Training set size: {len(X_train)} samples ({len(X_train)/len(X)*100:.1f}%)")
# print(f"Test set size: {len(X_test)} samples ({len(X_test)/len(X)*100:.1f}%)")
# print(f"Number of features: {X_train.shape[1]}")

cm = confusion_matrix(y_test, y_pred, labels=None)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=idxLst, yticklabels=idxLst)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()